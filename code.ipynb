{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "972e9f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "mlflow.set_experiment(\"Country_Clustering\")\n",
    "\n",
    "# Ensure any previously active run is ended\n",
    "mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run(run_name=\"Load_Dataset\"):\n",
    "    df = pd.read_csv(\"Country-data.csv\")\n",
    "\n",
    "    mlflow.log_param(\"rows\", df.shape[0])\n",
    "    mlflow.log_param(\"columns\", df.shape[1])\n",
    "\n",
    "    # Save and log the dataset\n",
    "    df.to_csv(\"country_data_logged.csv\", index=False)\n",
    "    mlflow.log_artifact(\"country_data_logged.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b71abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Country-data.csv\")\n",
    "\n",
    "# Start MLflow run\n",
    "mlflow.set_experiment(\"Country_Clustering\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"Data_Exploration\"):\n",
    "\n",
    "    # âœ… Log simple params\n",
    "    mlflow.log_param(\"rows\", df.shape[0])\n",
    "    mlflow.log_param(\"columns\", df.shape[1])\n",
    "    mlflow.log_param(\"missing_values\", df.isnull().sum().sum())\n",
    "    mlflow.log_param(\"duplicates\", df.duplicated().sum())\n",
    "\n",
    "    # âœ… Save df.info() to a text file\n",
    "    with open(\"df_info.txt\", \"w\") as f:\n",
    "        df.info(buf=f)\n",
    "\n",
    "    # âœ… Save df.describe() to a CSV file\n",
    "    df.describe().to_csv(\"df_describe.csv\")\n",
    "\n",
    "    # âœ… Log files as artifacts\n",
    "    mlflow.log_artifact(\"df_info.txt\")\n",
    "    mlflow.log_artifact(\"df_describe.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ead47ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(\"Country_Clustering\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"EDA_Plots\"):\n",
    "\n",
    "    top_gdpp = df[['country','gdpp']].sort_values(by='gdpp', ascending=False).head(10)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.barh(top_gdpp['country'], top_gdpp['gdpp'], color='teal')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(\"Top 10 Countries by GDP per Capita\")\n",
    "    plt.xlabel(\"GDP per Capita\")\n",
    "    plt.savefig(\"top_gdpp.png\")  \n",
    "    mlflow.log_artifact(\"top_gdpp.png\")  \n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    top_income = df[['country','income']].sort_values(by='income', ascending=False).head(10)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.barh(top_income['country'], top_income['income'], color='orange')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(\"Top 10 Countries by Income\")\n",
    "    plt.xlabel(\"Income\")\n",
    "    plt.savefig(\"top_income.png\")\n",
    "    mlflow.log_artifact(\"top_income.png\")\n",
    "    plt.close()\n",
    "\n",
    "    low_life = df[['country','life_expec']].sort_values(by='life_expec').head(10)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.barh(low_life['country'], low_life['life_expec'], color='red')\n",
    "    plt.title(\"Bottom 10 Countries by Life Expectancy\")\n",
    "    plt.xlabel(\"Life Expectancy\")\n",
    "    plt.savefig(\"low_life.png\")\n",
    "    mlflow.log_artifact(\"low_life.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    top_child_mort = df[['country','child_mort']].sort_values(by='child_mort', ascending=False).head(10)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.barh(top_child_mort['country'], top_child_mort['child_mort'], color='purple')\n",
    "    plt.title(\"Top 10 Countries by Child Mortality\")\n",
    "    plt.xlabel(\"Child Mortality\")\n",
    "    plt.savefig(\"top_child_mort.png\")\n",
    "    mlflow.log_artifact(\"top_child_mort.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da064fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "\n",
    "with mlflow.start_run(run_name=\"Correlation_Heatmap\"):\n",
    "\n",
    "    num_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(num_df.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation Heatmap (Numeric Columns)\")\n",
    "\n",
    "    heatmap_file = \"correlation_heatmap.png\"\n",
    "    plt.savefig(heatmap_file)\n",
    "    plt.close()  \n",
    "\n",
    "    mlflow.log_artifact(heatmap_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58844d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness before log transform:\n",
      " inflation     5.154049\n",
      "exports       2.445824\n",
      "income        2.231480\n",
      "gdpp          2.218051\n",
      "imports       1.905276\n",
      "child_mort    1.450774\n",
      "total_fer     0.967092\n",
      "health        0.705746\n",
      "life_expec   -0.970996\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\PYTHON\\ML PROJECT 1\\venv\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skewness after log transform:\n",
      " inflation    -1.300086\n",
      "exports      -1.088961\n",
      "income       -0.235823\n",
      "gdpp          0.006548\n",
      "imports      -1.822794\n",
      "child_mort    0.066160\n",
      "dtype: float64\n",
      "âœ… Skewness analysis and histograms logged to MLflow\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "mlflow.set_experiment(\"Country_Clustering\")\n",
    "\n",
    "# End any active run to avoid MLflow active run errors\n",
    "mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run(run_name=\"Skewness_Analysis\"):\n",
    "\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(\"Country-data.csv\")\n",
    "    mlflow.log_param(\"rows\", df.shape[0])\n",
    "    mlflow.log_param(\"columns\", df.shape[1])\n",
    "\n",
    "    # Select numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['float64','int64'])\n",
    "\n",
    "    # Compute skewness before transformation\n",
    "    skewness_before = numeric_cols.skew().sort_values(ascending=False)\n",
    "    print(\"Skewness before log transform:\\n\", skewness_before)\n",
    "    \n",
    "    # Log skewness as metrics (optional, numeric only)\n",
    "    for col, val in skewness_before.items():\n",
    "        mlflow.log_metric(f\"{col}_skew_before\", val)\n",
    "\n",
    "    # Columns to log-transform\n",
    "    skewed_cols = ['inflation','exports','income','gdpp','imports','child_mort']\n",
    "    df[skewed_cols] = df[skewed_cols].apply(lambda x: np.log1p(x))\n",
    "\n",
    "    # Compute skewness after transformation\n",
    "    skewness_after = df[skewed_cols].skew()\n",
    "    print(\"\\nSkewness after log transform:\\n\", skewness_after)\n",
    "\n",
    "    # Log skewness after transformation\n",
    "    for col, val in skewness_after.items():\n",
    "        mlflow.log_metric(f\"{col}_skew_after\", val)\n",
    "\n",
    "    # Plot histograms of numeric columns\n",
    "    num_cols = df.select_dtypes(include=['float64','int64']).columns\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    for i, col in enumerate(num_cols, 1):\n",
    "        plt.subplot(4, 4, i)  # 4x4 grid for 13 columns\n",
    "        sns.histplot(df[col], kde=True, bins=30, color=\"lightgreen\")\n",
    "        plt.title(f\"{col}\\nSkew={df[col].skew():.2f}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save histogram figure and log to MLflow\n",
    "    hist_file = \"histograms.png\"\n",
    "    plt.savefig(hist_file)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(hist_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4823d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run(run_name=\"Scaling_Data\"):\n",
    "\n",
    "    # Scale numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df[numeric_cols])\n",
    "    df_scaled = pd.DataFrame(scaled_data, columns=numeric_cols)\n",
    "\n",
    "    # Save and log scaled dataset\n",
    "    scaled_file = \"new_scaled.csv\"\n",
    "    df_scaled.to_csv(scaled_file, index=False)\n",
    "    mlflow.log_artifact(scaled_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "69b5a5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# End any active MLflow run\n",
    "mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run(run_name=\"PCA_Explained_Variance\"):\n",
    "\n",
    "    # Fit PCA on scaled data\n",
    "    pca = PCA()\n",
    "    pca.fit(scaled_data)\n",
    "    cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "    # Plot cumulative explained variance\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(range(1, len(cum_var)+1), cum_var, marker='o', linestyle='-')\n",
    "    plt.axhline(y=0.9, color='r', linestyle='--', label='90% Variance Threshold')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('PCA - Explained Variance vs Components')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save plot and log as MLflow artifact\n",
    "    pca_plot_file = \"pca_cumulative_variance.png\"\n",
    "    plt.savefig(pca_plot_file)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(pca_plot_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f86312d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 PC1       PC2       PC3       PC4       PC5\n",
      "child_mort  0.436729  0.079424 -0.042111 -0.011543  0.172663\n",
      "exports    -0.244853  0.590777 -0.282626  0.112337  0.264484\n",
      "health     -0.144216 -0.070811  0.733609  0.611466  0.066185\n",
      "imports    -0.093832  0.751405  0.144931  0.093922 -0.307174\n",
      "income     -0.422480 -0.097914 -0.221052  0.086442  0.328513\n",
      "inflation   0.212637 -0.117902 -0.541510  0.755003 -0.256602\n",
      "life_expec -0.402127 -0.174776 -0.073659 -0.027508 -0.275990\n",
      "total_fer   0.392296  0.104261  0.053114  0.112371  0.603032\n",
      "gdpp       -0.424742 -0.100723 -0.093904  0.116830  0.433492\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# End any active MLflow run\n",
    "mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run(run_name=\"PCA_5_Loadings\"):\n",
    "\n",
    "    # Perform PCA with 5 components\n",
    "    pca5 = PCA(n_components=5)\n",
    "    X_pca5 = pca5.fit_transform(scaled_data)\n",
    "\n",
    "    # Create loadings DataFrame\n",
    "    loadings = pd.DataFrame(\n",
    "        pca5.components_.T,\n",
    "        columns=[f'PC{i+1}' for i in range(5)],\n",
    "        index=df.drop(columns=['country']).columns\n",
    "    )\n",
    "\n",
    "    # Print loadings\n",
    "    print(loadings)\n",
    "\n",
    "    # Save loadings as CSV and log as artifact\n",
    "    loadings_file = \"pca5_loadings.csv\"\n",
    "    loadings.to_csv(loadings_file, index=True)\n",
    "    mlflow.log_artifact(loadings_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0271dafe",
   "metadata": {},
   "source": [
    "ðŸ”¹ Cluster 1 â†’ Developed, High GDP & Health\n",
    "\n",
    "High income, GDP, life expectancy\n",
    "\n",
    "Low child mortality, low fertility\n",
    "\n",
    "Example: USA, Germany, Japan\n",
    "\n",
    "ðŸ”¹ Cluster 2 â†’ Poor, High Mortality & Fertility\n",
    "\n",
    "Low GDP, low income, low life expectancy\n",
    "\n",
    "High child mortality, high fertility\n",
    "\n",
    "Example: Sub-Saharan African nations\n",
    "\n",
    "ðŸ”¹ Cluster 3 â†’ Trade-Oriented Economies\n",
    "\n",
    "High imports & exports (big contribution from PC2)\n",
    "\n",
    "GDP may vary, but trade dominates economy\n",
    "\n",
    "Example: Singapore, UAE, Netherlands\n",
    "\n",
    "ðŸ”¹ Cluster 4 â†’ Middle-Income / Emerging Economies\n",
    "\n",
    "Moderate GDP & income\n",
    "\n",
    "Improving life expectancy\n",
    "\n",
    "Still some child mortality & fertility issues\n",
    "\n",
    "Example: India, Brazil, South Africa\n",
    "\n",
    "ðŸ”¹ Cluster 5 â†’ High Inflation / Economic Instability\n",
    "\n",
    "Strong loading from PC3 & PC5 (inflation factor)\n",
    "\n",
    "Economy unstable â†’ despite income, poor health balance\n",
    "\n",
    "Example: Argentina, Venezuela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7af83975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of PCA components used: 5\n"
     ]
    }
   ],
   "source": [
    "# Assuming you're still inside the MLflow run\n",
    "num_components = X_pca5.shape[1]\n",
    "print(\"Number of PCA components used:\", num_components)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2985721a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC1: ['child_mort', 'gdpp', 'income']\n",
      "PC2: ['imports', 'exports', 'life_expec']\n",
      "PC3: ['health', 'inflation', 'exports']\n",
      "PC4: ['inflation', 'health', 'gdpp']\n",
      "PC5: ['total_fer', 'gdpp', 'income']\n"
     ]
    }
   ],
   "source": [
    "# End any active MLflow run\n",
    "mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run(run_name=\"PCA_Top_Features\"):\n",
    "\n",
    "    # Perform PCA with 5 components (if not already done)\n",
    "    from sklearn.decomposition import PCA\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    pca5 = PCA(n_components=5)\n",
    "    X_pca5 = pca5.fit_transform(scaled_data)\n",
    "\n",
    "    # Create loadings dataframe\n",
    "    loadings = pd.DataFrame(\n",
    "        pca5.components_.T,\n",
    "        columns=[f'PC{i+1}' for i in range(5)],\n",
    "        index=df.drop(columns=['country']).columns\n",
    "    )\n",
    "\n",
    "    # Function to get top features per PC\n",
    "    def top_features_per_pc(loadings, n=3):\n",
    "        top_features_dict = {}\n",
    "        for pc in loadings.columns:\n",
    "            top_feats = loadings[pc].abs().sort_values(ascending=False).head(n).index.tolist()\n",
    "            top_features_dict[pc] = top_feats\n",
    "            print(f\"{pc}: {top_feats}\")\n",
    "        return top_features_dict\n",
    "\n",
    "    # Show and log top 3 features for each PC\n",
    "    top_feats_dict = top_features_per_pc(loadings, n=3)\n",
    "\n",
    "    # Save top features as CSV and log as artifact\n",
    "    top_feats_df = pd.DataFrame(top_feats_dict)\n",
    "    top_feats_file = \"pca_top_features.csv\"\n",
    "    top_feats_df.to_csv(top_feats_file, index=False)\n",
    "    mlflow.log_artifact(top_feats_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "83469c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# End any active MLflow run\n",
    "mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run(run_name=\"KMeans_Elbow_Plot\"):\n",
    "\n",
    "    # Compute SSE for different k\n",
    "    sse = []\n",
    "    K = range(2, 11)  # test clusters from 2 to 10\n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X_pca5)\n",
    "        sse.append(kmeans.inertia_)\n",
    "        # Optionally log SSE for each k\n",
    "        mlflow.log_metric(f\"sse_k_{k}\", kmeans.inertia_)\n",
    "\n",
    "    # Plot elbow curve\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(K, sse, 'o-', color='blue')\n",
    "    plt.xlabel(\"Number of clusters (k)\")\n",
    "    plt.ylabel(\"SSE (Inertia)\")\n",
    "    plt.title(\"Elbow Method for Optimal k\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save and log plot as artifact\n",
    "    elbow_plot_file = \"kmeans_elbow_plot.png\"\n",
    "    plt.savefig(elbow_plot_file)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(elbow_plot_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2078fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# End any active MLflow run\n",
    "mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run(run_name=\"KDistance_Plot\"):\n",
    "\n",
    "    # Fit NearestNeighbors\n",
    "    neigh = NearestNeighbors(n_neighbors=5)\n",
    "    nbrs = neigh.fit(scaled_data)\n",
    "    distances, indices = nbrs.kneighbors(scaled_data)\n",
    "\n",
    "    # Sort distances for 4th neighbor (k-distance)\n",
    "    distances = np.sort(distances[:, 4])\n",
    "\n",
    "    # Plot k-distance graph\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(distances, marker='o', linestyle='-')\n",
    "    plt.xlabel(\"Points sorted by distance\")\n",
    "    plt.ylabel(\"k-distance\")\n",
    "    plt.title(\"k-distance Graph for DBSCAN\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save and log plot as artifact\n",
    "    kdist_plot_file = \"k_distance_plot.png\"\n",
    "    plt.savefig(kdist_plot_file)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(kdist_plot_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "98f4532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps=0.5 â†’ clusters=0\n",
      "eps=1.0 â†’ clusters=2\n",
      "eps=2.0 â†’ clusters=1\n",
      "eps=3.0 â†’ clusters=1\n",
      "eps=5.0 â†’ clusters=1\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# End any active MLflow run\n",
    "mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run(run_name=\"DBSCAN_Cluster_Counts\"):\n",
    "\n",
    "    eps_values = [0.5, 1.0, 2.0, 3.0, 5.0]\n",
    "\n",
    "    for eps in eps_values:\n",
    "        db = DBSCAN(eps=eps, min_samples=5).fit(scaled_data)\n",
    "        labels = db.labels_\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "        # Print for reference\n",
    "        print(f\"eps={eps} â†’ clusters={n_clusters}\")\n",
    "\n",
    "        # Log number of clusters as MLflow metric\n",
    "        mlflow.log_metric(f\"dbscan_clusters_eps_{eps}\", n_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c1f5d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# End any active MLflow run\n",
    "mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run(run_name=\"Clustering_Comparison\"):\n",
    "\n",
    "    # --- Define clustering algorithms ---\n",
    "    kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "    hc = AgglomerativeClustering(n_clusters=5, metric='euclidean', linkage='ward')\n",
    "    dbscan = DBSCAN(eps=1.0, min_samples=5)\n",
    "\n",
    "    # --- Fit & Predict ---\n",
    "    labels = {\n",
    "        \"KMeans\": kmeans.fit_predict(scaled_data),\n",
    "        \"Hierarchical\": hc.fit_predict(scaled_data),\n",
    "        \"DBSCAN\": dbscan.fit_predict(scaled_data),\n",
    "    }\n",
    "\n",
    "    # --- Reduce dimensions for visualization ---\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    data_2d = pca.fit_transform(scaled_data)\n",
    "\n",
    "    # --- Plot clusters for each algorithm ---\n",
    "    for algo_name, algo_labels in labels.items():\n",
    "        plt.figure(figsize=(8,6))\n",
    "        unique_labels = set(algo_labels)\n",
    "        for lbl in unique_labels:\n",
    "            cluster_points = data_2d[algo_labels == lbl]\n",
    "            plt.scatter(cluster_points[:,0], cluster_points[:,1], label=f'Cluster {lbl}' if lbl != -1 else 'Noise', s=50)\n",
    "        plt.title(f\"{algo_name} Clustering (2D PCA)\")\n",
    "        plt.xlabel(\"PC1\")\n",
    "        plt.ylabel(\"PC2\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Save and log each cluster plot as MLflow artifact\n",
    "        plot_file = f\"{algo_name}_2D_clusters.png\"\n",
    "        plt.savefig(plot_file)\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(plot_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "242e74c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Method  Clusters Found  Silhouette Score  Davies-Bouldin  \\\n",
      "0        KMeans               5             0.232           1.133   \n",
      "1  Hierarchical               5             0.199           1.225   \n",
      "2        DBSCAN               2            -0.014           1.461   \n",
      "\n",
      "   ARI_vs_KMeans  \n",
      "0          1.000  \n",
      "1          0.491  \n",
      "2            NaN  \n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, davies_bouldin_score\n",
    "\n",
    "# End any active MLflow run\n",
    "mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run(run_name=\"Clustering_Comparison\"):\n",
    "\n",
    "    # --- Scale numeric data and handle NaNs ---\n",
    "    numeric_cols = df.drop(columns=['country']).select_dtypes(include=[np.number]).columns\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "    # Replace NaNs with column mean\n",
    "    col_means = np.nanmean(scaled_data, axis=0)\n",
    "    inds = np.where(np.isnan(scaled_data))\n",
    "    scaled_data[inds] = np.take(col_means, inds[1])\n",
    "\n",
    "    # --- KMeans ---\n",
    "    kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "    kmeans_labels = kmeans.fit_predict(scaled_data)\n",
    "    kmeans_sil = silhouette_score(scaled_data, kmeans_labels)\n",
    "    kmeans_dbi = davies_bouldin_score(scaled_data, kmeans_labels)\n",
    "    kmeans_clusters = len(set(kmeans_labels))\n",
    "    \n",
    "    mlflow.log_metric(\"KMeans_silhouette\", kmeans_sil)\n",
    "    mlflow.log_metric(\"KMeans_davies_bouldin\", kmeans_dbi)\n",
    "\n",
    "    # --- Hierarchical ---\n",
    "    hc = AgglomerativeClustering(n_clusters=5, metric=\"euclidean\", linkage=\"ward\")\n",
    "    hc_labels = hc.fit_predict(scaled_data)\n",
    "    hc_sil = silhouette_score(scaled_data, hc_labels)\n",
    "    hc_dbi = davies_bouldin_score(scaled_data, hc_labels)\n",
    "    hc_clusters = len(set(hc_labels))\n",
    "    hc_ari = adjusted_rand_score(kmeans_labels, hc_labels)\n",
    "\n",
    "    mlflow.log_metric(\"Hierarchical_silhouette\", hc_sil)\n",
    "    mlflow.log_metric(\"Hierarchical_davies_bouldin\", hc_dbi)\n",
    "    mlflow.log_metric(\"Hierarchical_ARI_vs_KMeans\", hc_ari)\n",
    "\n",
    "    # --- DBSCAN ---\n",
    "    dbscan = DBSCAN(eps=1.0, min_samples=5)\n",
    "    db_labels = dbscan.fit_predict(scaled_data)\n",
    "    db_clusters = len(set(db_labels)) - (1 if -1 in db_labels else 0)\n",
    "    \n",
    "    # Silhouette score cannot be computed if only 1 cluster or all noise\n",
    "    if db_clusters > 1:\n",
    "        db_sil = silhouette_score(scaled_data, db_labels)\n",
    "        db_dbi = davies_bouldin_score(scaled_data, db_labels)\n",
    "    else:\n",
    "        db_sil = np.nan\n",
    "        db_dbi = np.nan\n",
    "\n",
    "    mlflow.log_metric(\"DBSCAN_silhouette\", db_sil if not np.isnan(db_sil) else -1)\n",
    "    mlflow.log_metric(\"DBSCAN_davies_bouldin\", db_dbi if not np.isnan(db_dbi) else -1)\n",
    "\n",
    "    # --- Compile results ---\n",
    "    results = pd.DataFrame({\n",
    "        \"Method\": [\"KMeans\", \"Hierarchical\", \"DBSCAN\"],\n",
    "        \"Clusters Found\": [kmeans_clusters, hc_clusters, db_clusters],\n",
    "        \"Silhouette Score\": [round(kmeans_sil, 3), round(hc_sil, 3), round(db_sil, 3) if not np.isnan(db_sil) else None],\n",
    "        \"Davies-Bouldin\": [round(kmeans_dbi, 3), round(hc_dbi, 3), round(db_dbi, 3) if not np.isnan(db_dbi) else None],\n",
    "        \"ARI_vs_KMeans\": [1.0, round(hc_ari, 3), None]\n",
    "    })\n",
    "\n",
    "    # Log results as CSV artifact\n",
    "    results_file = \"clustering_results.csv\"\n",
    "    results.to_csv(results_file, index=False)\n",
    "    mlflow.log_artifact(results_file)\n",
    "\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c60b50",
   "metadata": {},
   "source": [
    "Rich & healthy\n",
    "\n",
    "Poor & unhealthy\n",
    "\n",
    "Trade-heavy\n",
    "\n",
    "Emerging middle\n",
    "\n",
    "Inflation/unstable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260127ff",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
